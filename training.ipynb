{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the training notebook for the Urban Sound Classification project (NAML-2024 project by Lorenzo Gentile). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure you have the following modules (the versions listed are the ones tested, may also work on others):\n",
    "- tensorflow 2.13\n",
    "- numpy 1.23.5\n",
    "- pandas 2.0.3\n",
    "- librosa 0.10.0\n",
    "- matplotlib 3.7.1\n",
    "- scikit-learn 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:33.930852Z",
     "start_time": "2023-05-02T15:46:33.925395Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import random\n",
    "import pathlib\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from IPython.display import Audio\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using two datasets for this project:\n",
    "- The UrbanSound8K dataset, which contains 8732 labeled sound samples from 10 different classes.\n",
    "- The ESC-50 dataset, which contains 2000 labeled sound samples from 50 different classes. Only 10 classes are urban sounds, (this subset is called ESC-10), but we will also test the model on the full ESC-50 dataset, to see how it performs on a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, we will define the functions to load the datasets.\n",
    "Since the 2 datasets have a different folder structure, we will define 2 separate functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sr = 16000\n",
    "\n",
    "def load_UrbanSound8K(path: pathlib.Path) -> (pd.DataFrame, tf.data.Dataset):\n",
    "\n",
    "    # Load the metadata\n",
    "    metadata_path = path / \"metadata/UrbanSound8K.csv\"\n",
    "    audio_path = path / \"audio\"\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Get the number of classes\n",
    "    num_classes = metadata['class'].nunique()\n",
    "\n",
    "    # Initialize lists to hold file paths and labels\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Shuffle the metadata entries to load samples in a random order\n",
    "    shuffled_metadata = metadata.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Iterate over the metadata entries\n",
    "    for _, row in shuffled_metadata.iterrows():\n",
    "        # Construct the file path for the current entry\n",
    "        fold_name = f'fold{row[\"fold\"]}'\n",
    "        file_name = row['slice_file_name']\n",
    "        file_path = audio_path / fold_name / file_name\n",
    "\n",
    "        # Append the file path and label to the lists\n",
    "        file_paths.append(str(file_path))\n",
    "        labels.append(row['classID'])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    file_paths_tensor = tf.convert_to_tensor(file_paths, dtype=tf.string)\n",
    "    labels_tensor = tf.convert_to_tensor(labels, dtype=tf.int64)\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    one_hot_labels_tensor = tf.one_hot(labels_tensor, num_classes)\n",
    "\n",
    "    # Create a dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths_tensor, one_hot_labels_tensor))\n",
    "\n",
    "    def load_audio_librosa(file_path, label):\n",
    "        # This function now expects a numpy array input for file_path and label\n",
    "        audio, _ = librosa.load(file_path.numpy(), sr=target_sr, mono=True)\n",
    "        return audio.astype(np.float32), np.array(label).astype(np.int64)\n",
    "    \n",
    "    # Wrap the load_audio_librosa function\n",
    "    def load_audio_wrapped(file_path, label):\n",
    "        [audio, label] = tf.py_function(load_audio_librosa, [file_path, label], [tf.float32, tf.int64])\n",
    "        return audio, label\n",
    "\n",
    "    # Map the wrapped load_audio function to the dataset\n",
    "    dataset = dataset.map(load_audio_wrapped, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return metadata, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'UrbanSound8K' # Choose between 'UrbanSound8K' and 'ESC-10'\n",
    "\n",
    "path_dict = {\n",
    "    'UrbanSound8K': pathlib.Path(\"UrbanSound8K\"),\n",
    "    'ESC-10': pathlib.Path(\"ESC-50-master\")\n",
    "}\n",
    "\n",
    "path = path_dict[DATASET_NAME]\n",
    "\n",
    "# Load the UrbanSound8K dataset\n",
    "metadata, audio_dataset = load_UrbanSound8K(path)\n",
    "\n",
    "# Construct class names array corresponding to the one-hot labels\n",
    "unique_classes = metadata[['classID', 'class']].drop_duplicates().sort_values('classID')\n",
    "class_names = unique_classes['class'].to_numpy()\n",
    "\n",
    "# Display the class names\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pad the audio samples to the maximum length in the whole dataset. This is done to ensure that all samples have the same length, which is required by the neural network, which accepts only fixed-size inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(audio, label):\n",
    "    max_length_seconds = (metadata['end'] - metadata['start']).max()\n",
    "    max_length_samples = int(target_sr * max_length_seconds)\n",
    "\n",
    "    current_length = tf.shape(audio)[0]\n",
    "    padding_amount = max_length_samples - current_length\n",
    "\n",
    "    # Use tf.cond to decide whether to pad or truncate\n",
    "    padded_audio = tf.cond(\n",
    "        padding_amount < 0,\n",
    "        lambda: audio[:max_length_samples],  # Truncate the audio\n",
    "        lambda: tf.pad(audio, paddings=[[0, padding_amount]], mode='CONSTANT', constant_values=0)  # Pad the audio\n",
    "    )\n",
    "\n",
    "    return padded_audio, label\n",
    "\n",
    "# Map the pad_with_zeros function to the dataset\n",
    "audio_dataset = audio_dataset.map(pad_with_zeros, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 6 audio clips in the training set\n",
    "\n",
    "for audio, label in audio_dataset.take(6):\n",
    "    print(f\"Audio shape: {audio.shape}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Class: {class_names[np.argmax(label)]}\")\n",
    "    display(Audio(audio, rate=target_sr))\n",
    "    plt.figure()\n",
    "    plt.plot(audio)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:34.932592Z",
     "start_time": "2023-05-02T15:46:34.777139Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Function for adding noises\n",
    "\n",
    "# #Load noise files\n",
    "# pink_noise_file = './_background_noise_/pink_noise.wav'\n",
    "# white_noise_file = './_background_noise_/white_noise.wav'\n",
    "\n",
    "# pink_noise_audio, _ = librosa.load(pink_noise_file, sr=16000)\n",
    "# white_noise_audio, _ = librosa.load(white_noise_file, sr=16000)\n",
    "\n",
    "# # Convert to tensors\n",
    "# white_noise_tensor = tf.convert_to_tensor(white_noise_audio, dtype=tf.float32)\n",
    "# pink_noise_tensor = tf.convert_to_tensor(pink_noise_audio, dtype=tf.float32)\n",
    "\n",
    "# # Add noise to audio\n",
    "# def add_noises(audio_tensor, noise_types=['pink'], noise_probs=[1], noise_levels=[0.01]):\n",
    "#     for noise_type, noise_prob, noise_level in zip(noise_types, noise_probs, noise_levels):\n",
    "#         if random.random() < noise_prob:\n",
    "#             if noise_type == 'white':\n",
    "#                 noise_tensor = white_noise_tensor\n",
    "#             elif noise_type == 'pink':\n",
    "#                 noise_tensor = pink_noise_tensor\n",
    "\n",
    "#             batch_size, audio_len = tf.shape(audio_tensor)[0], tf.shape(audio_tensor)[1]\n",
    "#             noise_len = int(tf.shape(noise_tensor)[0])\n",
    "#             start = tf.random.uniform((batch_size,), 0, noise_len - audio_len, dtype=tf.int32)\n",
    "#             indices = tf.expand_dims(tf.range(batch_size), axis=1)\n",
    "#             starts = tf.concat([indices, tf.expand_dims(start, axis=1)], axis=1)\n",
    "#             noise = tf.map_fn(lambda x: tf.slice(noise_tensor, [x[1]], [audio_len]), starts, dtype=tf.float32)\n",
    "#             audio_tensor = audio_tensor + noise_level * noise\n",
    "#     return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function for time shifting\n",
    "\n",
    "# def time_shift(audio_tensor, shift_range=0.1):\n",
    "#     batch_size, audio_len = tf.shape(audio_tensor)[0], tf.shape(audio_tensor)[1]\n",
    "#     shift_amount = tf.cast(tf.cast(audio_len, tf.float32) * shift_range, tf.int32)\n",
    "#     shift = tf.random.uniform((batch_size,), minval=-shift_amount, maxval=shift_amount, dtype=tf.int32)\n",
    "\n",
    "#     def shift_audio(x):\n",
    "#         audio, delta = x\n",
    "#         paddings = tf.cond(\n",
    "#             delta < 0,\n",
    "#             lambda: ((-delta, 0),),\n",
    "#             lambda: ((0, delta),)\n",
    "#         )\n",
    "#         return tf.pad(tf.slice(audio, [tf.math.maximum(0, delta)], [audio_len - tf.math.abs(delta)]), paddings)\n",
    "\n",
    "#     shifted_audio = tf.map_fn(shift_audio, (audio_tensor, shift), dtype=tf.float32)\n",
    "#     return shifted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function for pitch shifting\n",
    "\n",
    "# def pitch_shift(audio_tensor, pitch_range=(0.3, 0.3), sample_rate=16000):\n",
    "#     def shift_pitch(x):\n",
    "#         audio = x.numpy()\n",
    "#         factor = np.random.uniform(pitch_range[0], pitch_range[1])\n",
    "#         n_bins = 12  # You can adjust this value based on your requirements\n",
    "#         shifted_audio = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=factor, bins_per_octave=n_bins)\n",
    "#         return tf.convert_to_tensor(shifted_audio, dtype=tf.float32)\n",
    "\n",
    "#     return tf.py_function(shift_pitch, [audio_tensor], tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function for applying all the preprocessing steps\n",
    "# def preprocess_audio(audio, label, noise_colors=['pink'], noise_probs=[1], noise_levels=[0.01],\n",
    "#                      shift_range=0.1,\n",
    "#                      pitch_range=(-1, 1)):\n",
    "#     audio = time_shift(audio, shift_range)\n",
    "#     audio = add_noises(audio, noise_colors, noise_probs, noise_levels)\n",
    "#     audio = pitch_shift(audio, pitch_range)\n",
    "#     return audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOISE_COLORS = ['pink', 'white']\n",
    "# NOISE_PROBS = [0.5, 0.5]\n",
    "# NOISE_LEVELS = [0.01, 0.01]\n",
    "# SHIFT_RANGE = 0.1\n",
    "# PITCH_RANGE = (-1, 1)\n",
    "\n",
    "# # Apply the preprocessing function to your train_ds and val_ds\n",
    "\n",
    "# train_ds = train_ds.map(lambda audio, label: preprocess_audio(audio, label, NOISE_COLORS, NOISE_PROBS, NOISE_LEVELS, SHIFT_RANGE, PITCH_RANGE), tf.data.AUTOTUNE, deterministic=True)\n",
    "# val_ds = val_ds.map(lambda audio, label: preprocess_audio(audio, label, NOISE_COLORS, NOISE_PROBS, NOISE_LEVELS, SHIFT_RANGE, PITCH_RANGE), tf.data.AUTOTUNE, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectrogram parameters\n",
    "frame_length = 512\n",
    "frame_step = 160\n",
    "fft_length = 512\n",
    "\n",
    "#MFCC parameters\n",
    "num_mel_bins = 40\n",
    "num_mfccs = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:36.349389Z",
     "start_time": "2023-05-02T15:46:36.219330Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_spectrogram(waveform):\n",
    "  \n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(waveform, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "  \n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  # Add a channel dimension to the spectrogram. This is required for the Conv2D input layer, which expects a tensor of shape (batch_size, height, width, channels).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "def compute_mfcc(waveform):\n",
    "    # First, compute the spectrogram of the input waveform\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "\n",
    "    # Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    \n",
    "    # Compute the mel spectrogram\n",
    "    mel_spectrogram = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins=num_mel_bins,\n",
    "        num_spectrogram_bins=tf.shape(spectrogram)[-1],\n",
    "        sample_rate=target_sr,\n",
    "        lower_edge_hertz=20.0,  # Typically 20 Hz is used for the lower edge\n",
    "        upper_edge_hertz=target_sr / 2)  # Nyquist frequency\n",
    "\n",
    "    mel_spectrogram = tf.tensordot(spectrogram, mel_spectrogram, 1)\n",
    "    mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(mel_spectrogram.shape[-1:]))\n",
    "\n",
    "    # Compute the log mel spectrogram\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "\n",
    "    # Compute MFCCs from log mel spectrograms\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :num_mfccs]\n",
    "\n",
    "    # Add a channel dimension to the MFCCs\n",
    "    mfccs = mfccs[..., tf.newaxis]\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3 types of normalization\n",
    "\n",
    "def mean_std_normalize(image, label):\n",
    "    # Compute mean and standard deviation\n",
    "    mean = tf.reduce_mean(image)\n",
    "    std = tf.math.reduce_std(image)\n",
    "    # Standardize the image\n",
    "    image = (image - mean) / (std + 1e-6)\n",
    "    return image, label\n",
    "\n",
    "def min_max_normalize(image, label):\n",
    "    image = (image - tf.reduce_min(image)) / (tf.reduce_max(image) - tf.reduce_min(image))\n",
    "    return image, label\n",
    "\n",
    "def z_score_normalize(image, label):\n",
    "    image = (image - tf.reduce_mean(image)) / tf.math.reduce_std(image)\n",
    "    return image, label\n",
    "\n",
    "def per_channel_mean_std_normalize(image, label):\n",
    "    mean = tf.math.reduce_mean(image, axis=[0, 1])\n",
    "    std = tf.math.reduce_std(image, axis=[0, 1])\n",
    "    image = (image - mean) / std\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset based on chosen 2d representation and normalization\n",
    "def transform_normalize_dataset(dataset, transform, normalization):\n",
    "    if transform == 'spectrogram':\n",
    "        dataset = dataset.map(lambda audio, label: (compute_spectrogram(audio), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    elif transform == 'mfcc':\n",
    "        dataset = dataset.map(lambda audio, label: (compute_mfcc(audio), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid transform value. Choose either 'spectrogram' or 'mfcc'.\")\n",
    "\n",
    "    if normalization == 'mean_std':\n",
    "        dataset = dataset.map(mean_std_normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    elif normalization == 'min_max':\n",
    "        dataset = dataset.map(min_max_normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    elif normalization == 'z_score':\n",
    "        dataset = dataset.map(z_score_normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    elif normalization == 'per_channel_mean_std':\n",
    "        dataset = dataset.map(per_channel_mean_std_normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    elif normalization == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization value. Choose either 'mean_std', 'min_max', 'z_score', 'per_channel_mean_std', or 'none'.\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_TRANSFORM = 'mfcc' # Choose 'spectrogram' or 'mfcc'\n",
    "NORMALIZATION = 'per_channel_mean_std' # Choose 'mean_std', 'min_max', 'z_score', 'per_channel_mean_std', or 'none'\n",
    "\n",
    "dataset = transform_normalize_dataset(audio_dataset, AUDIO_TRANSFORM, NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 6 elements of the transformed dataset\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, (image, label) in enumerate(dataset.take(6)):\n",
    "\n",
    "    ax = plt.subplot(3, 2, i + 1)\n",
    "    data = tf.squeeze(image).numpy().T\n",
    "    img = plt.imshow(data, aspect='auto', cmap='inferno', origin='lower')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Class: {class_names[np.argmax(label)]}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add ticks\n",
    "    ax.set_xticks(range(0, data.shape[1], data.shape[1] // 5))  # Adjust the range and step as needed\n",
    "    ax.set_yticks(range(0, data.shape[0], data.shape[0] // 5))  # Adjust the range and step as needed\n",
    "    \n",
    "    # Show the colorbar\n",
    "    plt.colorbar(img, ax=ax)\n",
    "    \n",
    "    # Turn on axis\n",
    "    plt.axis('on')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, we split the data into training, validation, and test sets and we batch the data. Splitting coefficients and batch size are modifiable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle_buffer_size = len(metadata)\n",
    "\n",
    "train_split = 0.8\n",
    "validation_split = 0.1\n",
    "\n",
    "num_samples = len(metadata)\n",
    "num_train_samples = int(num_samples * train_split)\n",
    "num_val_samples = int(num_samples * validation_split)\n",
    "\n",
    "# Cache the dataset\n",
    "dataset = dataset.cache()\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset = dataset.take(num_train_samples)\n",
    "test_val_dataset = dataset.skip(num_train_samples)\n",
    "val_dataset = test_val_dataset.take(num_val_samples)\n",
    "test_dataset = test_val_dataset.skip(num_val_samples)\n",
    "\n",
    "# Batch the datasets\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# Fix the shape of the datasets (needed for accuracy metric to work during training, see https://github.com/tensorflow/tensorflow/issues/32912)\n",
    "def _fixup_shape(images, labels):\n",
    "    images.set_shape([None, None, None, 1]) # 2D images with 1 channel\n",
    "    labels.set_shape([None, 10]) # 10 classes for UrbanSound8K\n",
    "    return images, labels\n",
    "train_dataset = train_dataset.map(_fixup_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(_fixup_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(_fixup_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prefetch the datasets\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = metadata['classID'].nunique()\n",
    "input_shape = next(iter(train_dataset))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "num_classes = metadata['classID'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the first convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 2), activation='tanh', input_shape=input_shape[1:]))\n",
    "model.add(tf.keras.layers.MaxPooling2D((9, 1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "# Add the second convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='tanh'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((3, 1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "# Flatten the output of the convolutional layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Add the last dense layer\n",
    "model.add(tf.keras.layers.Dense(num_classes))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.005),\n",
    "              loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                        mode=\"min\", patience=5,\n",
    "                                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[tensorboard_callback, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODELS_DIR = \"./saved_models/\"\n",
    "\n",
    "\n",
    "formatted_accuracy = \"{:.2f}\".format(test_accuracy * 100)\n",
    "model_path = pathlib.Path(SAVED_MODELS_DIR) / DATASET_NAME / f\"{AUDIO_TRANSFORM}-{NORMALIZATION}-{formatted_accuracy}\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_and_load_model(saved_models_dir, dataset_name, audio_transform, normalization):\n",
    "    # Construct the base directory path for the models\n",
    "    base_model_dir = pathlib.Path(saved_models_dir) / dataset_name\n",
    "    \n",
    "    # Check if the base model directory exists\n",
    "    if not base_model_dir.exists():\n",
    "        print(f\"No saved models found in {base_model_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # List all subdirectories in the base model directory\n",
    "    model_subdirs = [d for d in os.listdir(base_model_dir) if os.path.isdir(base_model_dir / d)]\n",
    "    \n",
    "    # Filter subdirectories based on the audio_transform and normalization\n",
    "    filtered_model_subdirs = [d for d in model_subdirs if d.startswith(f\"{audio_transform}-{normalization}\")]\n",
    "    \n",
    "    # Sort the models by the accuracy embedded in the directory name\n",
    "    filtered_model_subdirs.sort(key=lambda x: float(x.split('-')[-1]), reverse=True)\n",
    "    \n",
    "    # Print the available models with an index\n",
    "    for idx, model_subdir in enumerate(filtered_model_subdirs):\n",
    "        print(f\"{idx}: {model_subdir}\")\n",
    "    \n",
    "    # Ask the user to select a model to load\n",
    "    selected_index = int(input(\"Enter the index of the model to load: \"))\n",
    "    selected_model_subdir = filtered_model_subdirs[selected_index]\n",
    "    \n",
    "    # Load the selected model\n",
    "    full_model_path = base_model_dir / selected_model_subdir\n",
    "    model = tf.keras.models.load_model(full_model_path)\n",
    "    \n",
    "    print(f\"Loaded model from {full_model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model = list_and_load_model(SAVED_MODELS_DIR, 'UrbanSound8K', 'spectrogram', 'mean_std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end we can calculate the confusion matrix of our model and all the intermediate outputs of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix function\n",
    "def get_true_and_predicted_labels(model, dataset):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for image_batch, label_batch in dataset:\n",
    "        y_true.append(tf.argmax(label_batch, axis=-1))  # Convert one-hot encoded labels to class indices\n",
    "        preds = model.predict(image_batch, verbose=0)\n",
    "        y_pred.append(np.argmax(preds, axis=-1))\n",
    "\n",
    "    correct_labels = tf.concat([item for item in y_true], axis=0)\n",
    "    predicted_labels = tf.concat([item for item in y_pred], axis=0)\n",
    "\n",
    "    return correct_labels, predicted_labels\n",
    "\n",
    "def print_confusion_matrix(model, val_spectrogram_ds):\n",
    "    correct_labels, predicted_labels = get_true_and_predicted_labels(model, val_spectrogram_ds)\n",
    "    confusion_mtx = tf.math.confusion_matrix(correct_labels, predicted_labels).numpy()\n",
    "\n",
    "     # Optionally, you can use ConfusionMatrixDisplay from scikit-learn to visualize the confusion matrix\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    display = ConfusionMatrixDisplay(confusion_mtx, display_labels=class_names)\n",
    "\n",
    "    display.plot(xticks_rotation='vertical', ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_confusion_matrix(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the output of a layer\n",
    "\n",
    "def get_layer_output(model, layer_index, test_dataset: tf.data.Dataset):\n",
    "\n",
    "    # Create a new model with the specified layer's output\n",
    "    layer_output_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[layer_index].output)\n",
    "\n",
    "    # Pass the input_data to the new model to get the output of the specified layer\n",
    "    example, label = test_dataset.rebatch(1).shuffle(len(test_dataset)).take(1).as_numpy_iterator().next()    \n",
    "\n",
    "    #convert label to string\n",
    "    label = class_names[np.argmax(label)]\n",
    "\n",
    "    #get layer output\n",
    "    layer_output = layer_output_model.predict(example)\n",
    "\n",
    "    return layer_output, label\n",
    "\n",
    "#functions to plot feature maps\n",
    "\n",
    "show_colorbar = True\n",
    "\n",
    "def plot_feature_maps(model, layer_index, test_dataset: tf.data.Dataset):\n",
    "\n",
    "    #handle plotting of input\n",
    "    if layer_index == -1:\n",
    "        example, label = test_dataset.rebatch(1).shuffle(len(test_dataset)).take(1).as_numpy_iterator().next()\n",
    "        label = class_names[np.argmax(label)]\n",
    "        plot_conv_feature_maps(example, 1, layer_index, model, label)\n",
    "        return\n",
    "\n",
    "    feature_maps, label = get_layer_output(model, layer_index, test_dataset)\n",
    "    \n",
    "    # Check the dimensions of the layer output\n",
    "    #if(\"CustomQuantizeLayer\" in str(type(model.layers[layer_index]))):\n",
    "    #    print(\"Specified layers is a CustomQuantizeLayer. Cannot plot feature maps.\")\n",
    "    #    return\n",
    "    if len(feature_maps.shape) == 4:  # Conv2D or MaxPooling2D layers\n",
    "        num_feature_maps = feature_maps.shape[-1]\n",
    "        plot_conv_feature_maps(feature_maps, num_feature_maps, layer_index, model, label)\n",
    "        \n",
    "    elif len(feature_maps.shape) == 2:  # Dense layer\n",
    "        plot_dense_feature_maps(feature_maps, layer_index, model, label)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Layer {layer_index} has an unsupported output shape. Cannot plot feature maps.\")\n",
    "\n",
    "def plot_conv_feature_maps(feature_maps, num_feature_maps, layer_index, model, label, show_colorbar=show_colorbar):\n",
    "    # Create a grid of subplots\n",
    "    if num_feature_maps == 1:\n",
    "        num_cols = 1\n",
    "        num_rows = 1\n",
    "        figsize = (10, 7)  # Rectangular dimensions for single-channel input spectrogram\n",
    "    else:\n",
    "        num_cols = 4\n",
    "        num_rows = num_feature_maps // num_cols + (num_feature_maps % num_cols > 0)\n",
    "        figsize = (15, 15)  # Square dimensions for multi-channel feature maps\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "\n",
    "    # Set title of the figure as the layer name and index\n",
    "    if (layer_index != -1) & (layer_index != 0): fig.suptitle(f'Feature Maps of Layer {layer_index}: {model.layers[layer_index].name}\\nOutput shape {feature_maps.shape}\\nLabel: {label}')\n",
    "    elif layer_index == -1 : fig.suptitle(f'Input Spectrogram\\nLabel: {label}')\n",
    "    elif layer_index == 0 : fig.suptitle(f'Quantized Input Spectrogram\\nLabel: {label}')\n",
    "    \n",
    "    # Plot each feature map\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            idx = i * num_cols + j\n",
    "            if idx < num_feature_maps:\n",
    "                if num_feature_maps == 1:\n",
    "                    ax = axes\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "                img = ax.imshow(feature_maps[0, :, :, idx], cmap='gray')\n",
    "                if (layer_index != -1) & (layer_index != 0): ax.set_title(f'Feature Map {idx}')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                # Add a colorbar legend to the right of each image if show_colorbar is True\n",
    "                if show_colorbar:\n",
    "                    cbar = fig.colorbar(img, ax=ax)\n",
    "                    cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_dense_feature_maps(feature_maps, layer_index, model, label):\n",
    "    # Reshape the feature maps to a 1D array\n",
    "    reshaped_feature_maps = np.reshape(feature_maps, (-1,))\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Set title of the figure as the layer name and index\n",
    "    fig.suptitle(f'Feature Maps of Layer {layer_index}: {model.layers[layer_index].name}\\nOutput shape {feature_maps.shape}\\nLabel: {label}')\n",
    "\n",
    "    # Plot the feature maps as a bar plot\n",
    "    ax.bar(range(len(reshaped_feature_maps)), reshaped_feature_maps)\n",
    "    ax.set_xlabel('Feature Map Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    #set x ticks to be label names if layer is last one \n",
    "    if(layer_index == len(model.layers) - 1):\n",
    "        ax.set_xticks(np.arange(len(reshaped_feature_maps)), class_names)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the output of a specific layer we select a random (but constant between executions) sample by setting a seed.\n",
    "\n",
    "To select a different layer, change the layer index in the last line of code (-1 shows the input sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(399)\n",
    "plot_feature_maps(model, 12, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print all layers activations\n",
    "\n",
    "def print_layers_activations(model: tf.keras.Sequential, test_ds: tf.data.Dataset, mode, index=None):\n",
    "    \n",
    "    if mode == \"all\":\n",
    "        for i in range(model.layers.__len__()):\n",
    "            print(f\"Layer {i}: {model.layers[i].name}\")\n",
    "            feature_maps, label = get_layer_output(model, i, test_ds)\n",
    "            print(feature_maps)\n",
    "            print(feature_maps.shape)\n",
    "            print(\"\\n\")\n",
    "    elif mode == \"single\":\n",
    "        if index == -1:\n",
    "            example, label = test_ds.rebatch(1).shuffle(len(test_ds)).take(1).as_numpy_iterator().next() \n",
    "            print(\"Spectrogram with label \" + class_names[np.argmax(label)])\n",
    "            print(\"\\n\")\n",
    "            print(example)\n",
    "            print(example.shape)\n",
    "            return\n",
    "        print(f\"Layer {index}: {model.layers[index].name}\")\n",
    "        feature_maps, label = get_layer_output(model, index, test_ds)\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        print(feature_maps)\n",
    "        print(feature_maps.shape)\n",
    "        print(\"\\n\")\n",
    "    print(class_names[np.argmax(label)])\n",
    "\n",
    "#Print all layers activations\n",
    "print_layers_activations(model, test_dataset, \"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d525f717c636b61dd10d03170c153b82d88c7b14443ea3adfa4837365d4574ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
